title: Deep Decompositional Network
date: 2015-7-1 22:42:57
tags: 
---
<!--more-->
####三层结构
![Imgur](http://i.imgur.com/6BCRMLr.png)
* 遮挡估算层
![Imgur](http://i.imgur.com/nbxRmI8.png)
![Imgur](http://i.imgur.com/h7mcEcO.png)
输出为0,1的map，
$$ 
x^o \in [0,1]^n 
$$
* 完成层
	分为4小层,矩阵和偏移如下
    ![Imgur](http://i.imgur.com/GqvTpnB.png)
    其中c1,c2为编码层，数据压缩，降噪，从高维映射到低维。
    c1',c2'为解码层，再映射回来。如下：
    ![Imgur](http://i.imgur.com/hJbd4fp.png)
    z是编码，xc是解码。
* 分解层
	就是标注，把特征映射到标签的过程。
    一共两小层。如图a右边，Wt1和t2. 也是使用类似于扩张的方法中对每个标签单独进行。如下图和下面那句话。
    ![Imgur](http://i.imgur.com/zXU3u8l.png)
####训练
看来这论文是把整个人体区域当输入干的。从代码表现来看，先压缩到固定尺寸，然后输出固定尺寸。所以在此一开始说的以下这些话。
![Imgur](http://i.imgur.com/quymeuW.png)
* 遮罩估计层的预训练
使用字典学习的方法，大约就是稀疏编码里训练的字典样子。公式一样，如下。
![Imgur](http://i.imgur.com/egZSd8C.png)
其中上面有横线的是遮挡的groundtruth。
常量b给省了。分解开来就是下面这个样子：
![Imgur](http://i.imgur.com/OS1VJAm.png)
* 完成层预训练
这层就是干了个数据重构，图像压了再扩，这样细节就没了。说是降噪了。
插RBM。RBM网络结构如下
![Imgur](http://i.imgur.com/n2bc6vD.png)
能量函数如下。
![Imgur](http://i.imgur.com/DweTSx3.png)
出自[这里](http://www.cnblogs.com/tornadomeet/archive/2013/03/27/2984725.html)
目标就是让能量函数最小。
插完。
这论文中的完成层参数预训练就是用的RBM网络。他的能量函数如下：
![Imgur](http://i.imgur.com/f20uG0F.png)
* 分解层预训练
这块没啥说的，就是两层矩阵。
####整体修整fine-tuning
目标函数
![Imgur](http://i.imgur.com/8Q3yG4r.png)
随机梯度下降
![Imgur](http://i.imgur.com/Ysl9xY8.png)
