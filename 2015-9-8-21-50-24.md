title: caffe
date: 2015-9-8 21:50:24
tags: caffe

---

[softmax](http://www.cnblogs.com/tornadomeet/archive/2013/03/22/2975978.html) 是logistic回归的多值的扩展。

* **forward and backward**
![Imgur](http://wangfan.net:9000/DrIjXL7.png)

 The Net::Forward() and Net::Backward() methods carry out the respective passes while Layer::Forward() and Layer::Backward() compute each step.
 The Solver optimizes a model by first calling forward to yield the output and loss, then calling backward to generate the gradient of the model, and then incorporating the gradient into a weight update that attempts to minimize the loss. 

<!--more-->

#[1.layer](http://caffe.berkeleyvision.org/tutorial/layers.html)

#### Vision Layers

Vision layers usually take images as input and produce other images as output.

c channel; w width; h height;

**卷积**

![Imgur](http://wangfan.net:9000/DDiDWEf.png)

**pooling**

![Imgur](http://wangfan.net:9000/gIrswiW.png)

**Local Response Normalization (LRN)**

![Imgur](http://wangfan.net:9000/9BsJaa3.png)

####Loss Layers

Loss drives learning by comparing an output to a target and assigning cost to minimize. 
![Imgur](http://wangfan.net:9000/6OsNJul.png)

[Hinge loss](https://en.wikipedia.org/wiki/Hinge_loss)

* Sigmoid Cross-Entropy

* SigmoidCrossEntropyLoss

* Infogain

* InfogainLoss

* Accuracy and Top-k

####Activation / Neuron Layers

 taking one bottom blob and producing one top blob of the same size.
 
 * ReLU / Rectified-Linear and Leaky-ReLU : Given an input value x, The ReLU layer computes the output as x if x > 0 and negative_slope * x if x <= 0. 
 * Sigmoid
 * TanH / Hyperbolic Tangent : tan(x)
 * Absolute Value
 * power
 * BNLL :  log(1 + exp(x)) 

####Data Layers

Data enters Caffe through data layers: they lie at the bottom of nets.

####Common Layers

*  InnerProduct
*  Splitting
*  Flattening
*  Reshape
*  Concatenation
*  Slicing
*  Elementwise Operations
*  Argmax
*  Softmax
*  Mean-Variance Normalization (MVN)

#[2.Solver](http://caffe.berkeleyvision.org/tutorial/solver.html)

The solver orchestrates model optimization by coordinating the network’s forward inference and backward gradients to form parameter updates that attempt to improve the loss. 

####Stochastic gradient descent(SGD)

![Imgur](http://wangfan.net:9000/O0bo26W.png)
![Imgur](http://wangfan.net:9000/X8GOMdE.png)

####AdaGrad

![Imgur](http://wangfan.net:9000/3iYAmnT.png)

####Nesterov’s accelerated gradient(NAG)

![Imgur](http://wangfan.net:9000/O1nQA3A.png)

#[3.Interfaces](http://caffe.berkeleyvision.org/tutorial/interfaces.html)

####Command Line
